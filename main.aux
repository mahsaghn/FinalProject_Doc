\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\providecommand*\new@tpo@label[2]{}
\HyPL@Entry{0<</S/D>>}
\providecommand \oddpage@label [2]{}
\citation{Hierarchical-Attention-Network}
\citation{stance_persian}
\citation{stance_persian}
\citation{stance_persian}
\citation{stance_persian}
\citation{parsfever}
\citation{parsbert}
\citation{parsbert}
\citation{albert}
\citation{stance_persian}
\citation{stance_persian}
\citation{adasyn}
\citation{memory_network}
\citation{UCLMR}
\citation{UCLMR}
\citation{takestancefake}
\citation{book_fake}
\citation{stance_robust}
\citation{takestancefake}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{15}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{stanceCI}
\citation{stanceCI}
\citation{memory_network}
\citation{stance_robust}
\citation{book_datafake}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{17}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Tokenization}{17}{section.2.1}\protected@file@percent }
\newlabel{lr:tokenization}{{2.1}{17}{Tokenization}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}NLTK}{17}{subsection.2.1.1}\protected@file@percent }
\citation{bert}
\citation{bow}
\citation{tfidf}
\citation{word2vec}
\citation{bow}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Hazm}{18}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Stanza}{18}{subsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}WordPiece}{18}{subsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Word Representation}{18}{section.2.2}\protected@file@percent }
\newlabel{lr:wordrep}{{2.2}{18}{Word Representation}{section.2.2}{}}
\newlabel{fn:hazm}{{2}{18}{Hazm}{subsection.2.1.2}{}}
\citation{tfidf}
\citation{word2vec}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}BoW}{19}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}TF-IDF}{19}{subsection.2.2.2}\protected@file@percent }
\newlabel{eq:tf}{{2.1}{19}{TF-IDF}{equation.2.2.1}{}}
\newlabel{idf}{{2.2}{19}{TF-IDF}{equation.2.2.2}{}}
\newlabel{tfidf}{{2.3}{19}{TF-IDF}{equation.2.2.3}{}}
\citation{book_fake}
\citation{book_fake}
\citation{GNbayes}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}W2V}{20}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Machine Learning}{20}{section.2.3}\protected@file@percent }
\newlabel{lr:ml}{{2.3}{20}{Machine Learning}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Gaussian Naive Bayes}{20}{subsection.2.3.1}\protected@file@percent }
\citation{svc}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Support Vector Machines}{21}{subsection.2.3.2}\protected@file@percent }
\newlabel{SVM}{{2.3.2}{21}{Support Vector Machines}{subsection.2.3.2}{}}
\citation{randomforest}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}LinearSVC}{22}{subsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Random Forest}{22}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Logistic Regression}{22}{subsection.2.3.5}\protected@file@percent }
\newlabel{eq:logil}{{2.4}{22}{Logistic Regression}{equation.2.3.4}{}}
\newlabel{eq:logill}{{2.5}{22}{Logistic Regression}{equation.2.3.5}{}}
\newlabel{eq:logisel}{{2.6}{22}{Logistic Regression}{equation.2.3.6}{}}
\citation{smothe}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Oversampling}{23}{section.2.4}\protected@file@percent }
\newlabel{lr:oversampling}{{2.4}{23}{Oversampling}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}RandomOverSampler}{23}{subsection.2.4.1}\protected@file@percent }
\citation{svmsmothe}
\citation{svmsmothe}
\citation{borderlinesmothe}
\citation{borderlinesmothe}
\citation{adasyn}
\citation{adasyn}
\citation{bert}
\citation{bert}
\citation{bert}
\citation{spotfake}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}SMOTE}{24}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}SVM-SMOTE }{24}{subsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}BorderlineSMOTE}{24}{subsection.2.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.5}ADASYN}{24}{subsection.2.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Language Representation Model}{24}{section.2.5}\protected@file@percent }
\newlabel{lr:lm}{{2.5}{24}{Language Representation Model}{section.2.5}{}}
\citation{book_datafake}
\citation{parsbert}
\citation{bert}
\citation{albert}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}BERT}{25}{subsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}ParsBERT}{25}{subsection.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}ALBERT}{25}{subsection.2.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Evaluation Metrics}{26}{section.2.6}\protected@file@percent }
\newlabel{lr:evalmetrics}{{2.6}{26}{Evaluation Metrics}{section.2.6}{}}
\newlabel{eq:acc}{{2.7}{26}{Evaluation Metrics}{equation.2.6.7}{}}
\newlabel{eq:f1}{{2.8}{26}{Evaluation Metrics}{equation.2.6.8}{}}
\citation{Augenstein2016StanceDW}
\citation{Augenstein2016StanceDW}
\citation{Augenstein2016StanceDW}
\citation{svc}
\citation{bow}
\citation{UCLMR}
\citation{sikit-learn}
\citation{UCLMR}
\citation{UCLMR}
\citation{Hierarchical-Attention-Network}
\citation{Hierarchical-Attention-Network}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Related Works}{27}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{literature}{{3}{27}{Related Works}{chapter.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Schematic diagram of UCLMR’s system.\relax }}{28}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:UCLMR-system}{{3.1}{28}{Schematic diagram of UCLMR’s system.\relax }{figure.caption.5}{}}
\citation{Hierarchical-Attention-Network}
\citation{Hierarchical-Attention-Network}
\citation{Hierarchical-Attention-Network}
\citation{Hierarchical-Attention-Network}
\citation{Hierarchical-Attention-Network}
\citation{memory_network}
\citation{memory_network}
\citation{memory_network}
\citation{stance_robust}
\citation{bert}
\citation{stance_robust}
\citation{stance_robust}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Overview of \cite  {Hierarchical-Attention-Network} model.\relax }}{30}{figure.caption.6}\protected@file@percent }
\newlabel{fig:hierarchical_att}{{3.2}{30}{Overview of \cite {Hierarchical-Attention-Network} model.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Architecture of Memory Network for the stance classification task.\relax }}{30}{figure.caption.7}\protected@file@percent }
\newlabel{fig:mem_network}{{3.3}{30}{Architecture of Memory Network for the stance classification task.\relax }{figure.caption.7}{}}
\citation{stance_persian}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Methodology}{33}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{sec:exp}{{4}{33}{Methodology}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Preprocessing}{33}{section.4.1}\protected@file@percent }
\newlabel{mth:preproces}{{4.1}{33}{Preprocessing}{section.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Features}{33}{section.4.2}\protected@file@percent }
\newlabel{mth:features}{{4.2}{33}{Features}{section.4.2}{}}
\citation{stance_persian}
\citation{stance_persian}
\citation{stance_persian}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Similarity}{34}{subsection.4.2.1}\protected@file@percent }
\newlabel{eq_ratio}{{4.1}{34}{Similarity}{equation.4.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Root Distance}{34}{subsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}ImportantWords}{34}{subsection.4.2.3}\protected@file@percent }
\citation{stance_persian}
\citation{persent}
\citation{persent}
\citation{persent}
\citation{persent}
\citation{persent}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Is Question}{35}{subsection.4.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Has Two Parts}{35}{subsection.4.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.6}Polarity}{35}{subsection.4.2.6}\protected@file@percent }
\newlabel{eq_polar}{{4.2}{35}{Polarity}{equation.4.2.2}{}}
\citation{persent}
\citation{polar_hotel}
\citation{polar_servic}
\citation{parsfever}
\citation{fever}
\citation{parsfever}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Balancing}{36}{section.4.3}\protected@file@percent }
\newlabel{mth:balance}{{4.3}{36}{Balancing}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Extending dataset}{36}{subsection.4.3.1}\protected@file@percent }
\citation{stance_persian}
\citation{parsfever}
\citation{stance_persian}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Oversampling and Undersampling}{37}{subsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Tune model parameters}{37}{subsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Machine Learning}{37}{section.4.4}\protected@file@percent }
\newlabel{mth:ml}{{4.4}{37}{Machine Learning}{section.4.4}{}}
\citation{book_datafake}
\citation{book_fake}
\citation{stance_robust}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Schematic of each machine learning model.\relax }}{38}{figure.caption.8}\protected@file@percent }
\newlabel{fig:mlschm}{{4.1}{38}{Schematic of each machine learning model.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Deep Learning}{38}{section.4.5}\protected@file@percent }
\newlabel{mth:dl}{{4.5}{38}{Deep Learning}{section.4.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Article to Claim}{38}{section.4.6}\protected@file@percent }
\newlabel{mth:a2c}{{4.6}{38}{Article to Claim}{section.4.6}{}}
\citation{stance_persian}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Schematic of our deep learning model.\relax }}{39}{figure.caption.9}\protected@file@percent }
\newlabel{fig:dlschm}{{4.2}{39}{Schematic of our deep learning model.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Schematic of our fake news detection model.\relax }}{39}{figure.caption.10}\protected@file@percent }
\newlabel{fig:fnschm}{{4.3}{39}{Schematic of our fake news detection model.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Fake News Detection}{39}{section.4.7}\protected@file@percent }
\newlabel{mth:fn}{{4.7}{39}{Fake News Detection}{section.4.7}{}}
\newlabel{sec:fakenews}{{4.7}{39}{Fake News Detection}{section.4.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Value of credibility according to GroundTruth and Veracity labels. \relax }}{40}{table.caption.11}\protected@file@percent }
\newlabel{tbl:cred}{{4.1}{40}{Value of credibility according to GroundTruth and Veracity labels. \relax }{table.caption.11}{}}
\newlabel{eq:cred}{{4.3}{40}{Fake News Detection}{equation.4.7.3}{}}
\newlabel{eq:honesty}{{4.4}{40}{Fake News Detection}{equation.4.7.4}{}}
\citation{stance_persian}
\citation{stance_persian}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Experiments}{43}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter:four}{{5}{43}{Experiments}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Dataset}{43}{section.5.1}\protected@file@percent }
\newlabel{sec:dataset}{{5.1}{43}{Dataset}{section.5.1}{}}
\citation{stance_persian}
\citation{stance_persian}
\citation{stance_persian}
\citation{stance_persian}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Samples distribution in four classes of Persian stance dataset.\relax }}{45}{table.caption.12}\protected@file@percent }
\newlabel{tbl:sdatapie}{{5.1}{45}{Samples distribution in four classes of Persian stance dataset.\relax }{table.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Comparison between article-to-claim and headline-to-claim labels, samples distribution in \cite  {stance_persian} dataset.\relax }}{45}{figure.caption.13}\protected@file@percent }
\newlabel{fig:datacom}{{5.1}{45}{Comparison between article-to-claim and headline-to-claim labels, samples distribution in \cite {stance_persian} dataset.\relax }{figure.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Fake news data set statistics\relax }}{45}{table.caption.14}\protected@file@percent }
\newlabel{fake-news-dataset}{{5.2}{45}{Fake news data set statistics\relax }{table.caption.14}{}}
\newlabel{tbl:fakedata}{{5.2}{45}{Fake news data set statistics\relax }{table.caption.14}{}}
\citation{stance_persian}
\citation{stance_persian}
\citation{stance_persian}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Claim veracity label's distribution in \cite  {stance_persian} dataset.\relax }}{46}{figure.caption.15}\protected@file@percent }
\newlabel{fig:fake}{{5.2}{46}{Claim veracity label's distribution in \cite {stance_persian} dataset.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Tokenizetion}{46}{section.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Comparison performance of \textit  {Hazm}, \textit  {Stanford}, \textit  {NLTK} and \textit  {BERT} tokenizers.\relax }}{47}{figure.caption.16}\protected@file@percent }
\newlabel{fig:tekenres}{{5.3}{47}{Comparison performance of \textit {Hazm}, \textit {Stanford}, \textit {NLTK} and \textit {BERT} tokenizers.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Comparison duration of tokenizing algorithm on \cite  {stance_persian} dataset.\relax }}{47}{figure.caption.17}\protected@file@percent }
\newlabel{fig:tokentime}{{5.4}{47}{Comparison duration of tokenizing algorithm on \cite {stance_persian} dataset.\relax }{figure.caption.17}{}}
\citation{stance_persian}
\citation{svc}
\citation{stance_persian}
\citation{bow}
\citation{tfidf}
\citation{word2vec}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Stop-Words}{48}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Word Representation}{48}{section.5.4}\protected@file@percent }
\newlabel{fn:kharazi}{{4}{48}{Stop-Words}{section.5.3}{}}
\citation{bow}
\citation{word2vec}
\citation{svc}
\citation{randomforest}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Comparison accuracy of SVM model with different configuration of stop words.\relax }}{49}{figure.caption.18}\protected@file@percent }
\newlabel{fig:stopwords}{{5.5}{49}{Comparison accuracy of SVM model with different configuration of stop words.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Predictors}{49}{section.5.5}\protected@file@percent }
\newlabel{sec:predictors}{{5.5}{49}{Predictors}{section.5.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Comparison SVM model with Bow, TF-iDF and Word2Vec word representaion algorithms.\relax }}{50}{figure.caption.19}\protected@file@percent }
\newlabel{fig:wordrep}{{5.6}{50}{Comparison SVM model with Bow, TF-iDF and Word2Vec word representaion algorithms.\relax }{figure.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Comparison of accuracy score and F1-score with different combinations of predictors for both SVM and Random Forest classifiers.\relax }}{51}{table.caption.20}\protected@file@percent }
\newlabel{tlb:predictors}{{5.3}{51}{Comparison of accuracy score and F1-score with different combinations of predictors for both SVM and Random Forest classifiers.\relax }{table.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Machine Learning}{51}{section.5.6}\protected@file@percent }
\newlabel{sec:ml}{{5.6}{51}{Machine Learning}{section.5.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}SVM}{51}{subsection.5.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Tuning SVM model parameters with TF-iDF representation algorithms\relax }}{52}{figure.caption.21}\protected@file@percent }
\newlabel{fig:svm}{{5.7}{52}{Tuning SVM model parameters with TF-iDF representation algorithms\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}Linear SVC}{52}{subsection.5.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Tuning LinearSVC model parameters with TF-iDF representation algorithms.\relax }}{53}{figure.caption.22}\protected@file@percent }
\newlabel{fig:linearsvm}{{5.8}{53}{Tuning LinearSVC model parameters with TF-iDF representation algorithms.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.3}Random Forest}{53}{subsection.5.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Random Forest machine learning model different configuration on stance detection task. Type of line presents type of the boundary applied on each model. Solid line, dash line and doted line stands for no boundary, sqrt of total feature and log2 of total feature respectively.\relax }}{54}{figure.caption.23}\protected@file@percent }
\newlabel{fig:randomforest}{{5.9}{54}{Random Forest machine learning model different configuration on stance detection task. Type of line presents type of the boundary applied on each model. Solid line, dash line and doted line stands for no boundary, sqrt of total feature and log2 of total feature respectively.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.4}Logistic Regression}{54}{subsection.5.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Effect of $\rho $ parameter of \textit  {elasticnet} penalty on stance detection task.\relax }}{55}{figure.caption.24}\protected@file@percent }
\newlabel{fig:logistic1}{{5.10}{55}{Effect of $\rho $ parameter of \textit {elasticnet} penalty on stance detection task.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Effect of regression parameter of \textit  {elasticnet} penalty on stance detection task.\relax }}{55}{figure.caption.25}\protected@file@percent }
\newlabel{fig:logistic2}{{5.11}{55}{Effect of regression parameter of \textit {elasticnet} penalty on stance detection task.\relax }{figure.caption.25}{}}
\citation{stance_persian}
\citation{stance_persian}
\citation{parsfever}
\citation{stance_persian}
\citation{parsfever}
\citation{stance_persian}
\citation{stance_persian}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Comprasion of Logistic Regression machine learning models.\relax }}{56}{figure.caption.26}\protected@file@percent }
\newlabel{fig:logistic3}{{5.12}{56}{Comprasion of Logistic Regression machine learning models.\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.5}Comparison}{56}{subsection.5.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Dataset Balancing}{56}{section.5.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces Comparison SVM model between Bow, TF-iDF and Word2Vec word representation algorithms.\relax }}{57}{figure.caption.27}\protected@file@percent }
\newlabel{fig:all}{{5.13}{57}{Comparison SVM model between Bow, TF-iDF and Word2Vec word representation algorithms.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces Comparison between article-to-claim and headline-to-claim labels, samples distribution in \cite  {stance_persian} dataset, after extending by \cite  {parsfever} .\relax }}{57}{figure.caption.28}\protected@file@percent }
\newlabel{fig:datab1}{{5.14}{57}{Comparison between article-to-claim and headline-to-claim labels, samples distribution in \cite {stance_persian} dataset, after extending by \cite {parsfever} .\relax }{figure.caption.28}{}}
\citation{parsfever}
\citation{stance_persian}
\citation{bert}
\citation{parsbert}
\citation{albert}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces Comparison SVM model with Bow, TF-iDF and Word2Vec word representation algorithms.\relax }}{58}{figure.caption.29}\protected@file@percent }
\newlabel{fig:balanc}{{5.15}{58}{Comparison SVM model with Bow, TF-iDF and Word2Vec word representation algorithms.\relax }{figure.caption.29}{}}
\citation{parsbert}
\citation{parsbert}
\citation{albert}
\citation{parsbert}
\citation{parsbert}
\citation{albert}
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Deep Learning}{59}{section.5.8}\protected@file@percent }
\newlabel{sec:dl}{{5.8}{59}{Deep Learning}{section.5.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.9}Article to Claim}{59}{section.5.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces Deep learning procedure on the headline-to-claim stance detection task. Left figures illustrate loss score and right figures illustrate accuracy score of train and test data during the training procedure. (a, b) Pre-trained language model based on Google's BERT (\cite  {parsbert}) on Persian corpus (c, d) Pre-trained monolingual language model based on ParsBERT (\cite  {parsbert}) on Persian corpus. (e, f) Pre-trained language model based on ALBERT (\cite  {albert}) on Persian corpus\relax }}{60}{figure.caption.30}\protected@file@percent }
\newlabel{fig:deep}{{5.16}{60}{Deep learning procedure on the headline-to-claim stance detection task. Left figures illustrate loss score and right figures illustrate accuracy score of train and test data during the training procedure. (a, b) Pre-trained language model based on Google's BERT (\cite {parsbert}) on Persian corpus (c, d) Pre-trained monolingual language model based on ParsBERT (\cite {parsbert}) on Persian corpus. (e, f) Pre-trained language model based on ALBERT (\cite {albert}) on Persian corpus\relax }{figure.caption.30}{}}
\citation{stance_persian}
\citation{stance_persian}
\citation{adasyn}
\citation{stance_persian}
\citation{stance_persian}
\citation{adasyn}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Comparison of headline-to-claim stance detection models.\relax }}{61}{table.caption.31}\protected@file@percent }
\newlabel{tbl:allstance}{{5.4}{61}{Comparison of headline-to-claim stance detection models.\relax }{table.caption.31}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Comparison between article-to-claim machine learning and deep learning models.\relax }}{61}{table.caption.32}\protected@file@percent }
\newlabel{tbl:a2c}{{5.5}{61}{Comparison between article-to-claim machine learning and deep learning models.\relax }{table.caption.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.10}Fake News}{61}{section.5.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces Left figures illustrate loss score and right figures illustrate accuracy score of train and test data during training procedure for each iteration. (a, b) Training procedure on fake news detection model trained on the \cite  {stance_persian} dataset. (c, d) Training procedure on fake news detection model trained on oversampled \cite  {stance_persian} dataset by ADASYN (\cite  {adasyn}) algorithm.\relax }}{62}{figure.caption.33}\protected@file@percent }
\newlabel{fig:fakenews}{{5.17}{62}{Left figures illustrate loss score and right figures illustrate accuracy score of train and test data during training procedure for each iteration. (a, b) Training procedure on fake news detection model trained on the \cite {stance_persian} dataset. (c, d) Training procedure on fake news detection model trained on oversampled \cite {stance_persian} dataset by ADASYN (\cite {adasyn}) algorithm.\relax }{figure.caption.33}{}}
\citation{stance_persian}
\citation{parsbert}
\citation{book_fake}
\citation{stance_persian}
\citation{stance_persian}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{63}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{book_disinformation}
\citation{stance_persian}
\bibdata{main.bib}
\bibcite{Augenstein2016StanceDW}{{1}{2016}{{Augenstein et~al.}}{{Augenstein, Rockt{\"a}schel, Vlachos, and Bontcheva}}}
\bibcite{svc}{{2}{2011}{{Chang and Lin}}{{}}}
\bibcite{smothe}{{3}{2002}{{Chawla et~al.}}{{Chawla, Bowyer, Hall, and Kegelmeyer}}}
\bibcite{persent}{{4}{2016}{{Dashtipour et~al.}}{{Dashtipour, Hussain, Zhou, Gelbukh, Hawalah, and Cambria}}}
\bibcite{book_datafake}{{5}{2021}{{Deepak~P}}{{}}}
\bibcite{polar_hotel}{{6}{2019}{{Dehkharghani}}{{}}}
\bibcite{bert}{{7}{2019}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{takestancefake}{{8}{2019}{{Dulhanty et~al.}}{{Dulhanty, Deglint, Ben~Daya, and Wong}}}
\bibcite{sikit-learn}{{9}{2011}{{F.~Pedregosa and Duchesnay.}}{{}}}
\bibcite{parsbert}{{10}{2020}{{Farahani et~al.}}{{Farahani, Gharachorloo, Farahani, and Manthouri}}}
\bibcite{book_fake}{{11}{2021}{{Giansiracusa}}{{}}}
\bibcite{borderlinesmothe}{{12}{2005}{{Han et~al.}}{{Han, Wang, and Mao}}}
\bibcite{bow}{{13}{1954}{{Harris}}{{}}}
\bibcite{adasyn}{{14}{2008}{{He et~al.}}{{He, Bai, Garcia, and Li}}}
\bibcite{GNbayes}{{15}{1995}{{John and Langley}}{{}}}
\bibcite{albert}{{16}{2020}{{Lan et~al.}}{{Lan, Chen, Goodman, Gimpel, Sharma, and Soricut}}}
\bibcite{randomforest}{{17}{2002}{{Liaw and Wiener}}{{}}}
\bibcite{stance_persian}{{18}{2019}{{Majid~Zarharan}}{{}}}
\bibcite{polar_servic}{{19}{2020}{{Mehrdad~Farahani}}{{}}}
\bibcite{memory_network}{{20}{2018}{{Mohtarami et~al.}}{{Mohtarami, Baly, Glass, Nakov, M{\`a}rquez, and Moschitti}}}
\bibcite{stanceCI}{{21}{2017}{{Mrowca et~al.}}{{Mrowca, Wang, and Kosson}}}
\bibcite{UCLMR}{{22}{2017}{{Riedel et~al.}}{{Riedel, Augenstein, Spithourakis, and Riedel}}}
\bibcite{tfidf}{{23}{2010}{{Sammut and Webb}}{{}}}
\bibcite{stance_robust}{{24}{2020}{{Schiller et~al.}}{{Schiller, Daxenberger, and Gurevych}}}
\bibcite{book_disinformation}{{25}{2020}{{Shu et~al.}}{{Shu, Wang, Lee, and Liu}}}
\bibcite{spotfake}{{26}{2019}{{Singhal et~al.}}{{Singhal, Shah, Chakraborty, Kumaraguru, and Satoh}}}
\bibcite{Hierarchical-Attention-Network}{{27}{2018}{{Sun et~al.}}{{Sun, Wang, Zhu, and Zhou}}}
\bibcite{fever}{{28}{2018}{{Thorne et~al.}}{{Thorne, Vlachos, Christodoulopoulos, and Mittal}}}
\bibcite{word2vec}{{29}{2013}{{Tomas~Mikolov}}{{}}}
\bibcite{svmsmothe}{{30}{2003}{{Wu and Chang}}{{}}}
\bibcite{parsfever}{{31}{2021}{{Zarharan et~al.}}{{Zarharan, Ghaderan, Pourdabiri, Sayedi, Minaei-Bidgoli, Eetemadi, and Pilehvar}}}
\bibstyle{acl_natbib}
