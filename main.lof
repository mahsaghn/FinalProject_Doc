\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Schematic diagram of \cite {UCLMR} model.\relax }}{30}{figure.caption.6}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Overview of \cite {Hierarchical-Attention-Network} model.\relax }}{32}{figure.caption.7}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Architecture of \cite {memory_network} for the stance classification task.\relax }}{32}{figure.caption.8}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Schematic of each machine learning model.\relax }}{40}{figure.caption.9}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Schematic of our deep learning model.\relax }}{40}{figure.caption.10}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Schematic of our fake news detection model.\relax }}{41}{figure.caption.11}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Comparison between \ac {A2C} and \ac {H2C} labels, samples distribution in \cite {stance_persian} dataset.\relax }}{47}{figure.caption.14}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces Claim veracity label's distribution in \cite {stance_persian} dataset.\relax }}{48}{figure.caption.16}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Comparison performance of \textit {Hazm}, \textit {Stanza(Stanford)}, \textit {NLTK} and \textit {WordPiece(\ac {BERT})} tokenizers.\relax }}{49}{figure.caption.17}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces Comparison duration of tokenizing algorithm on \cite {stance_persian} dataset.\relax }}{49}{figure.caption.18}%
\contentsline {figure}{\numberline {5.5}{\ignorespaces Comparison accuracy of \ac {SVM} model with different configuration of stop words.\relax }}{51}{figure.caption.19}%
\contentsline {figure}{\numberline {5.6}{\ignorespaces Comparison \ac {SVM} model with \ac {BoW}, \ac {TFIDF} and \ac {W2V} word representaion algorithms.\relax }}{52}{figure.caption.20}%
\contentsline {figure}{\numberline {5.7}{\ignorespaces Tuning \ac {SVM} model parameters with \ac {TFIDF} representation algorithms\relax }}{54}{figure.caption.22}%
\contentsline {figure}{\numberline {5.8}{\ignorespaces Tuning LinearSVC model parameters with \ac {TFIDF} representation algorithms.\relax }}{55}{figure.caption.23}%
\contentsline {figure}{\numberline {5.9}{\ignorespaces Random Forest \ac {ML} model different configuration on stance detection task. Type of line presents type of the boundary applied on each model. Solid line, dash line and doted line stands for no boundary, sqrt of total feature and log2 of total feature respectively.\relax }}{56}{figure.caption.24}%
\contentsline {figure}{\numberline {5.10}{\ignorespaces Effect of $\rho $ parameter of \textit {elasticnet} penalty on stance detection task.\relax }}{57}{figure.caption.25}%
\contentsline {figure}{\numberline {5.11}{\ignorespaces Effect of regression parameter of \textit {elasticnet} penalty on stance detection task.\relax }}{57}{figure.caption.26}%
\contentsline {figure}{\numberline {5.12}{\ignorespaces Comprasion of Logistic Regression \ac {ML} models.\relax }}{58}{figure.caption.27}%
\contentsline {figure}{\numberline {5.13}{\ignorespaces Comparison \ac {SVM} model between \ac {BoW}, \ac {TFIDF} and \ac {W2V} word representation algorithms.\relax }}{59}{figure.caption.28}%
\contentsline {figure}{\numberline {5.14}{\ignorespaces Comparison between \ac {A2C} and \ac {H2C} labels, samples distribution in \cite {stance_persian} dataset, after extending by \cite {parsfever} .\relax }}{59}{figure.caption.29}%
\contentsline {figure}{\numberline {5.15}{\ignorespaces Comparison \ac {SVM} model with \ac {BoW}, \ac {TFIDF} and \ac {W2V} word representation algorithms.\relax }}{60}{figure.caption.30}%
\contentsline {figure}{\numberline {5.16}{\ignorespaces Deep learning procedure on the \ac {H2C} stance detection task. Left figures illustrate loss score and right figures illustrate accuracy score of train and test data during the training procedure. (a, b) Pre-trained language model based on Google's \ac {BERT} (\cite {parsbert}) on Persian corpus (c, d) Pre-trained monolingual language model based on \ac {ParsBERT} (\cite {parsbert}) on Persian corpus. (e, f) Pre-trained language model based on \ac {ALBERT} (\cite {albert}) on Persian corpus\relax }}{62}{figure.caption.31}%
\contentsline {figure}{\numberline {5.17}{\ignorespaces Left figures illustrate loss score and right figures illustrate accuracy score of train and test data during training procedure for each iteration. (a, b) Training procedure on fake news detection model trained on the \cite {stance_persian} dataset. (c, d) Training procedure on fake news detection model trained on oversampled \cite {stance_persian} dataset by \ac {ADASYN} (\cite {adasyn}) algorithm.\relax }}{64}{figure.caption.34}%
\addvspace {10\p@ }
