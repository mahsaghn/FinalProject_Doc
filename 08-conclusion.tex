We have evaluated Machine learning models on the Persian stance detection task as a baseline. Multiple predictors are extracted and different combinations of them are applied on machine learning models to find out the most effective predictor combination. Due to imbalanced samples distribution in the \cite{stance_persian} dataset, extending the dataset by \ac{ParsBERT} dataset (\cite{parsbert}) and oversampling methods are discussed and compared to each other in this project.

In the Persian stance detection task, using deep language models boosts our model performance noticeably. \ac{BERT} and \ac{ParsBERT} as its alternative have a high ability to make inferences from a given content so they can represent current content sufficiently and test accuracy in the model based on \ac{ParsBERT} language model is equal to 85.48\%. As the result,  \ac{ParsBERT} language models predict stance detection 10\% higher than machine learning algorithms on average. In contrast, requires time-consuming feature engineering and parameter tuning and they can't achieve high accuracy on language inference tasks. For both machine learning and deep learning models performance decrease on \ac{A2C} task. Despite this issue, machine learning algorithms have achieved an even higher than 70\% accuracy score on \ac{H2C} tasks. The black-box nature of machine learning algorithms means that nobody really knows why an AI lie detection system works as it does, nor what it is actually doing (\cite{book_fake}). 

The best pretrained stance detection model on \ac{H2C} and \ac{A2C} are separately utilized in fake news detection. We have achieved 99\% accuracy score on the \cite{stance_persian} dataset. Though, the number of samples in the dataset gathered by \cite{stance_persian} contains 1624 samples. Increasing number of samples helps the model to improves its generalization during the training procedure.

Adding attention layers to the classifier has improved stance classification results in current researches. For instance, \cite{book_disinformation} alleged that using attention layers in the model has made a great contribution to accuracy scores. Utilizing such models can also make an increase in the ability of these models to go beyond the current state in such low-resource settings. Besides, increasing the number of samples in the \cite{stance_persian} dataset toward achieving a balanced dataset can help the model to distinguish each class precisely. 
 
 
 