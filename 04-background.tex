
\section{BERT}
Pretrained language models have significant improve on many natural language model tasks such as natural language inference task (\cite{bert}). One of most impressive and powerful deep learning language model for text processing, is BERT which is stands for Bidirectional Encoder
Representations from Transformers (\cite{bert}) and developed by Google. And has received state of the art result due its previous researches (\cite{bert}). \cite{spotfake} improved its performance by make usage of the BERT language model in fake news detection. BERT base version contains 12 transformer blocks, containing 110 million parameters. There are also another variant of BERT such as large including 24 transformer blocks and multilingual BERT. BERT models are trained of Wikipedia corpus. 

One application of BERT is using first top layers of BERT model as contextual word embedding (\cite{book_datafake}). This model suggests a vector that representing a word which best fit in the current context.  BERT is a masked language model. It randomly masks some tokens and learn to predict masked tokens correctly. BERT utilizes bidirectional transformer, it means that BERT can inferred both left to right and right to left.

   
{\color{green}TODO}

\section{ALBERT}
{\color{green}TODO}

\section{ParsBERT}
{\color{green}TODO}